{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJdHPxXcg0UxG7aUgC+8T0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gracialukelo/fetch_dynamic_data/blob/main/web_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8F_iZwAr4mt"
      },
      "outputs": [],
      "source": [
        "# web_scraper.py\n",
        "\n",
        "# Installationsbefehle (entferne die Kommentarzeichen, wenn du sie ausführen möchtest)\n",
        "# pip install nest_asyncio playwright beautifulsoup4 fpdf\n",
        "\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "from bs4 import BeautifulSoup\n",
        "from fpdf import FPDF\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Damit wir async in Jupyter/Colab verwenden können\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Logging konfigurieren\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class WebScraper:\n",
        "    def __init__(self, url, output_file=\"extracted_teaser_data\", output_format=\"txt\", output_dir=\"data\"):\n",
        "        \"\"\"\n",
        "        Initialisiert die WebScraper-Klasse.\n",
        "\n",
        "        :param url: URL der zu scrapenden Webseite\n",
        "        :param output_file: Basisname der Ausgabedatei ohne Dateiendung\n",
        "        :param output_format: Format der Ausgabedatei ('txt', 'pdf', 'json', etc.)\n",
        "        :param output_dir: Verzeichnis, in dem die Ausgabedateien gespeichert werden sollen\n",
        "        \"\"\"\n",
        "        self.url = url\n",
        "        self.output_file = output_file\n",
        "        self.output_format = output_format.lower()\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)  # Erstelle den Ausgabeordner, falls er nicht existiert\n",
        "\n",
        "    async def fetch_page_content(self):\n",
        "        \"\"\"\n",
        "        Lädt die Webseite und gibt den HTML-Inhalt zurück.\n",
        "\n",
        "        :return: HTML-Inhalt der Webseite\n",
        "        \"\"\"\n",
        "        try:\n",
        "            async with async_playwright() as p:\n",
        "                browser = await p.chromium.launch(headless=True)\n",
        "                page = await browser.new_page()\n",
        "\n",
        "                # Webseite laden\n",
        "                logging.info(f\"Lade Webseite: {self.url}\")\n",
        "                await page.goto(self.url)\n",
        "\n",
        "                # Warte, bis die Seite vollständig geladen ist\n",
        "                await page.wait_for_load_state(\"networkidle\")\n",
        "\n",
        "                # Gesamten HTML-Inhalt der Seite erfassen\n",
        "                content = await page.content()\n",
        "\n",
        "                # Browser schließen\n",
        "                await browser.close()\n",
        "\n",
        "                logging.info(\"Webseiteninhalt erfolgreich abgerufen.\")\n",
        "                return content\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Fehler beim Abrufen der Seite: {e}\")\n",
        "            return None\n",
        "\n",
        "    def extract_important_attributes(self, html_content):\n",
        "        \"\"\"\n",
        "        Extrahiert wichtige Attribute aus dem HTML-Inhalt.\n",
        "\n",
        "        :param html_content: HTML-Inhalt der Webseite\n",
        "        :return: Liste der extrahierten Daten\n",
        "        \"\"\"\n",
        "        try:\n",
        "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "            data = []\n",
        "\n",
        "            # Suche nach spezifischen Elementen, die die relevanten Attribute enthalten\n",
        "            for element in soup.find_all(attrs={\"teaser-title\": True, \"teaser-description\": True, \"teaser-url\": True}):\n",
        "                # Extrahiere die Werte der gewünschten Attribute\n",
        "                teaser_title = element.get(\"teaser-title\")\n",
        "                teaser_description = element.get(\"teaser-description\")\n",
        "                teaser_url = element.get(\"teaser-url\")\n",
        "\n",
        "                # Füge die extrahierten Daten zur Liste hinzu\n",
        "                data.append({\"title\": teaser_title, \"description\": teaser_description, \"url\": teaser_url})\n",
        "\n",
        "            logging.info(\"Wichtige Attribute erfolgreich extrahiert.\")\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Fehler beim Extrahieren der Attribute: {e}\")\n",
        "            return []\n",
        "\n",
        "    def save_to_txt(self, data):\n",
        "        \"\"\"\n",
        "        Speichert die Daten als Textdatei.\n",
        "\n",
        "        :param data: Liste der zu speichernden Daten\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_path = self.output_dir / f\"{self.output_file}.txt\"\n",
        "            if file_path.exists():\n",
        "                logging.warning(f\"Die Datei '{file_path}' existiert bereits. Speichern wird übersprungen.\")\n",
        "                return\n",
        "\n",
        "            with file_path.open(\"w\", encoding=\"utf-8\") as file:\n",
        "                for item in data:\n",
        "                    file.write(f\"Title: {item['title']}\\n\")\n",
        "                    file.write(f\"Description: {item['description']}\\n\")\n",
        "                    file.write(f\"URL: {item['url']}\\n\")\n",
        "                    file.write(\"\\n\")  # Leerzeile zwischen den Einträgen\n",
        "\n",
        "            logging.info(f\"Daten wurden erfolgreich in '{file_path}' gespeichert.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Fehler beim Speichern der Daten als TXT: {e}\")\n",
        "\n",
        "    def save_to_pdf(self, data):\n",
        "        \"\"\"\n",
        "        Speichert die Daten als PDF-Datei.\n",
        "\n",
        "        :param data: Liste der zu speichernden Daten\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_path = self.output_dir / f\"{self.output_file}.pdf\"\n",
        "            if file_path.exists():\n",
        "                logging.warning(f\"Die Datei '{file_path}' existiert bereits. Speichern wird übersprungen.\")\n",
        "                return\n",
        "\n",
        "            pdf = FPDF()\n",
        "            pdf.add_page()\n",
        "\n",
        "            # Füge die Unicode-fähige Schriftart hinzu\n",
        "            pdf.add_font(\"DejaVu\", \"\", \"fonts/DejaVuSans.ttf\", uni=True)  # Stelle sicher, dass DejaVuSans.ttf im fonts/ Ordner ist\n",
        "            pdf.set_font(\"DejaVu\", \"\", 12)\n",
        "\n",
        "            for item in data:\n",
        "                title = item['title'] if item['title'] else ''\n",
        "                description = item['description'] if item['description'] else ''\n",
        "                url = item['url'] if item['url'] else ''\n",
        "\n",
        "                pdf.cell(200, 10, txt=f\"Title: {title}\", ln=True)\n",
        "                pdf.multi_cell(0, 10, txt=f\"Description: {description}\")\n",
        "                pdf.cell(200, 10, txt=f\"URL: {url}\", ln=True)\n",
        "                pdf.ln(5)  # Leerzeile zwischen den Einträgen\n",
        "\n",
        "            pdf.output(str(file_path))\n",
        "\n",
        "            logging.info(f\"Daten wurden erfolgreich in '{file_path}' gespeichert.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Fehler beim Speichern der Daten als PDF: {e}\")\n",
        "\n",
        "    def save_to_json(self, data):\n",
        "        \"\"\"\n",
        "        Speichert die Daten als JSON-Datei.\n",
        "\n",
        "        :param data: Liste der zu speichernden Daten\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_path = self.output_dir / f\"{self.output_file}.json\"\n",
        "            if file_path.exists():\n",
        "                logging.warning(f\"Die Datei '{file_path}' existiert bereits. Speichern wird übersprungen.\")\n",
        "                return\n",
        "\n",
        "            with file_path.open(\"w\", encoding=\"utf-8\") as file:\n",
        "                json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "            logging.info(f\"Daten wurden erfolgreich in '{file_path}' gespeichert.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Fehler beim Speichern der Daten als JSON: {e}\")\n",
        "\n",
        "    def save_data_to_file(self, data):\n",
        "        \"\"\"\n",
        "        Speichert die Daten im angegebenen Format.\n",
        "\n",
        "        :param data: Liste der zu speichernden Daten\n",
        "        \"\"\"\n",
        "        if self.output_format == \"txt\":\n",
        "            self.save_to_txt(data)\n",
        "        elif self.output_format == \"pdf\":\n",
        "            self.save_to_pdf(data)\n",
        "        elif self.output_format == \"json\":\n",
        "            self.save_to_json(data)\n",
        "        else:\n",
        "            logging.warning(f\"Unbekanntes Ausgabeformat '{self.output_format}'. Daten werden nicht gespeichert.\")\n",
        "\n",
        "    async def run(self):\n",
        "        \"\"\"\n",
        "        Führt den gesamten Scraping-Prozess aus.\n",
        "        \"\"\"\n",
        "        # HTML-Inhalt abrufen\n",
        "        content = await self.fetch_page_content()\n",
        "        if content is None:\n",
        "            logging.error(\"Keine Daten zum Verarbeiten. Beende das Programm.\")\n",
        "            return\n",
        "\n",
        "        # Wichtige Attribute extrahieren\n",
        "        extracted_data = self.extract_important_attributes(content)\n",
        "        if not extracted_data:\n",
        "            logging.warning(\"Keine Daten extrahiert.\")\n",
        "            return\n",
        "\n",
        "        # Daten speichern\n",
        "        self.save_data_to_file(extracted_data)\n"
      ]
    }
  ]
}