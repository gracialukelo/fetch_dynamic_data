{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlNuc9PZptn2I6aVUvyKRP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gracialukelo/fetch_dynamic_data/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOCGZ1w2sYjw"
      },
      "outputs": [],
      "source": [
        "# main.py\n",
        "\n",
        "# Installationsbefehle (entferne die Kommentarzeichen, wenn du sie ausführen möchtest)\n",
        "# pip install nest_asyncio playwright beautifulsoup4 fpdf google-api-python-client google-auth google-auth-httplib2 google-auth-oauthlib\n",
        "\n",
        "import asyncio\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from web_scraper import WebScraper  # Stelle sicher, dass web_scraper.py im gleichen Verzeichnis liegt\n",
        "from google_drive_uploader import GoogleDriveUploader  # Stelle sicher, dass google_drive_uploader.py im gleichen Verzeichnis liegt\n",
        "\n",
        "# Logging konfigurieren\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "async def main():\n",
        "    # WebScraper konfigurieren\n",
        "    url = \"https://sites.google.com/roche.com/site-mannheim-area-mgmt/site-area-management\"  # Ersetze dies durch die gewünschte URL\n",
        "    output_file = \"extracted_teaser_data\"\n",
        "    output_format = \"pdf\"  # Kann \"txt\", \"pdf\" oder \"json\" sein\n",
        "    output_dir = \"data\"  # Ausgabeordner\n",
        "\n",
        "    scraper = WebScraper(\n",
        "        url=url,\n",
        "        output_file=output_file,\n",
        "        output_format=output_format,\n",
        "        output_dir=output_dir\n",
        "    )\n",
        "\n",
        "    # Scraping-Prozess starten\n",
        "    await scraper.run()\n",
        "\n",
        "    # GoogleDriveUploader konfigurieren\n",
        "    SERVICE_ACCOUNT_FILE = 'service_account.json'\n",
        "    PARENT_FOLDER_ID = \"19l__xrAF6V9OkC0q327uh1Y0XARh1y8E\"  # Google Drive Ordner ID\n",
        "\n",
        "    uploader = GoogleDriveUploader(\n",
        "        service_account_file=SERVICE_ACCOUNT_FILE,\n",
        "        parent_folder_id=PARENT_FOLDER_ID\n",
        "    )\n",
        "\n",
        "    # Authentifizierung durchführen\n",
        "    try:\n",
        "        uploader.authenticate()\n",
        "    except Exception:\n",
        "        logging.critical(\"Programm wird beendet aufgrund eines Authentifizierungsfehlers.\")\n",
        "        return\n",
        "\n",
        "    # Alle Dateien im 'data' Ordner hochladen oder aktualisieren\n",
        "    uploaded_ids = uploader.upload_directory(output_dir, recursive=True)\n",
        "    if uploaded_ids:\n",
        "        logging.info(f\"{len(uploaded_ids)} Dateien wurden erfolgreich hochgeladen/aktualisiert.\")\n",
        "    else:\n",
        "        logging.warning(\"Es wurden keine Dateien hochgeladen/aktualisiert.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ]
    }
  ]
}